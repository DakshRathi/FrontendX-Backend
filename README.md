# FrontendX Performance Pal - Backend

![Language](https://img.shields.io/badge/Language-Python_3.11-blue.svg)
![Framework](https://img.shields.io/badge/Framework-FastAPI-green.svg)
![Containerization](https://img.shields.io/badge/Container-Docker-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

This repository contains the backend service for the **Performance Benchmarking Tool**, a submission for the FrontendX hackathon. The service analyzes any given website's frontend performance using the Google PageSpeed Insights API and provides AI-driven optimization tips through an interactive chat interface powered by the Groq LLM API.

## âœ¨ Features

-   **ğŸš€ Performance Analysis**: Get key frontend performance metrics for any website (mobile or desktop).
-   **ğŸ“Š Key Metrics**: Displays core web vitals like LCP, TBT, CLS, and the overall performance score.
-   **ğŸ¤– AI-Powered Suggestions**: Provides an initial, high-level performance summary generated by an LLM upon analysis.
-   **ğŸ’¬ Interactive Chat**: Engage in a continuous chat with an AI expert ("Pulse") to get detailed, context-aware optimization advice.
-   **ğŸ“„ Full Report Download**: An endpoint to download the complete raw JSON report from the Google PageSpeed Insights API.
-   **ğŸ³ Dockerized**: Fully containerized with Docker for easy setup and consistent deployment.

## ğŸ› ï¸ Tech Stack

-   **Backend**: Python, FastAPI
-   **AI/LLM**: Groq (using LLaMA 3), LangChain
-   **Performance Analysis**: Google PageSpeed Insights API
-   **Containerization**: Docker, Docker Compose
-   **Data Validation**: Pydantic
-   **Asynchronous HTTP Client**: HTTPX

## ğŸ“‚ Project Structure
```
FrontendX-backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ init.py 
â”‚ â”œâ”€â”€ main.py                   # FastAPI app, endpoints, and state management
â”‚ â”œâ”€â”€ models.py                 # Pydantic data models
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ init.py 
â”‚ â”‚ â”œâ”€â”€ pagespeed_service.py    # Logic for PageSpeed Insights API
â”‚ â”‚ â”œâ”€â”€ llm_service.py          # Logic for Groq LLM and LangChain
â”‚ â”‚ â””â”€â”€ processing_service.py   # Parsing and formatting pagespeed data
â”‚ â””â”€â”€ core/
â”‚   â”œâ”€â”€ init.py 
â”‚   â””â”€â”€ config.py               # Configuration and API key management
â”‚
â”œâ”€â”€ .env                        # Environment variables (API keys)
â”œâ”€â”€ .dockerignore 
â”œâ”€â”€ Dockerfile 
â””â”€â”€ requirements.txt 
```

## âš™ï¸ Setup and Installation

### Prerequisites

-   Git
-   Python 3.11+
-   Docker

### Local Setup Instructions

1.  **Clone the repository:**
    ```
    git clone https://github.com/DakshRathi/FrontendX-Backend.git
    cd FrontendX-Backend
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```
    python3 -m venv venv
    source venv/bin/activate
    # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```
    pip install -r requirements.txt
    ```

4.  **Create the environment file:**
    Create a file named `.env` in the project root and add your API keys.

    ```
    # .env

    # Get your API key from Google Cloud Console
    # https://developers.google.com/speed/docs/insights/get-started
    PAGESPEED_API_KEY="YOUR_GOOGLE_PAGESPEED_API_KEY"

    # Get your API key from GroqCloud
    # https://console.groq.com/keys
    GROQ_API_KEY="YOUR_GROQ_API_KEY"
    ```

## ğŸš€ Running the Application

You can run the application directly with Uvicorn or with Docker.

### Method 1: Locally with Uvicorn

This is useful for simple, direct testing.
```bash
uvicorn app.main:app --reload
```

The API will be available at `http://127.0.0.1:8000`.

### Method 2: Using Docker (Recommended)

This is the standard way to run the application, as it replicates a production-like environment.
```bash
docker build -t <username>/frontendx .
docker run -p 8000:8000 <username>/frontendx
```
The API will be available at `http://localhost:8000`. You can view the interactive API documentation (Swagger UI) at `http://localhost:8000/docs`.

## ğŸ“– API Endpoints

### 1. Analyze Website

Triggers a new performance analysis, gets an initial AI suggestion, and returns key metrics.

-   **Endpoint**: `POST /api/analyze`
-   **Request Body**:
    ```
    {
      "url": "https://www.google.com",
      "strategy": "desktop"
    }
    ```
-   **Success Response (200 OK)**:
    ```
    {
      "performance_score": 95,
      "metrics": [
        { "title": "First Contentful Paint", "value": "0.8 s" },
        { "title": "Largest Contentful Paint", "value": "1.2 s" }
      ],
      "initial_suggestion": "Hello! Your site shows excellent performance with a score of 95. To get even faster, you could look into optimizing the delivery of your JavaScript files. What would you like to explore first?"
    }
    ```

### 2. Chat with AI

Sends the chat history and the user's new query to the LLM for a streamed response.

-   **Endpoint**: `POST /api/chat`
-   **Request Body**:
    ```
    {
      "history": [
        { "role": "assistant", "content": "Hello! How can I help you optimize your site today?" },
        { "role": "user", "content": "Tell me more about the JavaScript execution time." }
      ]
    }
    ```
-   **Success Response (200 OK)**: A streaming response (`text/event-stream`) with chunks of the AI's answer.

### 3. Download Full Report

Allows the user to download the complete raw JSON data from the PageSpeed Insights API.

-   **Endpoint**: `GET /api/download-report`
-   **Response**: A JSON file download named `lighthouse_report.json`.

## ğŸ† Hackathon Submission Criteria

This backend was designed to help meet the following hackathon judging criteria:

-   **Code Quality and Modularity**: The code is structured into services, models, and a core configuration module, promoting separation of concerns.
-   **Creativity and Technical Challenge**: Integrates a powerful LLM (Groq) with a standard performance tool (Pagespeed) to create an innovative, interactive user experience.
-   **Documentation and Demo Readiness**: This README and the auto-generated FastAPI docs make the API easy to understand and use. The Dockerized setup ensures the demo is ready to run anywhere.

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

You are free to use, modify, and distribute this software in your own projects, provided you include the original copyright notice and permission notice in all copies or substantial portions of the Software.